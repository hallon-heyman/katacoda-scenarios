# How to use Splunk through a Docker container

![image](https://user-images.githubusercontent.com/62335201/116403937-70d68100-a82e-11eb-96c6-1382cf039134.png)

This tutorial will teach you how to use the tool Splunk enterprise through Docker. Docker is perhaps the worlds leading DevOps tool, it uses containerization technology to allow applications to be packaged as images and easily and efficiently run in containers. Splunk is a monitoring tool used by many companies to analyze data coming from various systems. The data that Splunk can for example be logs from your companies server, databases or applications. Splunk lets you search, visualize and analyze this data to find patterns discover bugs or malicious events. Using docker to set up Splunk is relatively easy and does not require you to install unnecessary software on your machine thanks to this tutorial being run through Katakoda. Remember that this instance of Splunk is entirely EPHEMERAL which is to say it will not remember anything you put in, when you close this tutorial all data that you uploaded to Splunk will be lost, likewise, feel free to play around in the playground of an online tutorial.
We will first introduce Docker, its history, and a summary of its underlying technology, and then move over to Splunk. If the user takes the time to fully read the background material accompanying the executable commands, the user will end this tutorial with a strong understanding of both Docker and Splunk, and how they are used together. We begin with a short introduction to Docker.  

## Introduction to Docker, the why and the how  

Many developers will already be familiar with Docker. If you feel confident in your understanding of Docker you can skip this part. Otherwise, or if you feel like you would like to refresh or deepen your knowledge of the docker technology, this background and the following first step will give you an adequate understanding of how Docker functions, why docker is so popular, and some basic usage of it.
The problem of server hosting
Docker is perhaps best introduced through its history - as a solution to a problem. The story begins in the days of the early internet when large companies and entities such as food chains, banks, etc; with new, large presences on the internet needed to maintaining hosting for their large and varied user needs. If these servers crashed the cost for the enterprise was potentially catastrophic, and different servers were required for the very varied user specifications. As such, server over-allocation was common and there was great inefficient great financial cost. Virtualization arose as a solution to this problem.

## Virtualization  

Virtualization as a concept and technology had already been around for decades, but it was a fringe functionality that was rarely used up till this point. In 2005, intel and AMD both added hardware-assisted support for virtualization, and new companies such as VMware started spreading the functionality to the wider market. Through virtualization, one host could run several OS or different applications and support a much wider range of different services to varied users, saving on required server infrastructure. Yet virtualization is costly. If you are unfamiliar with the structure of virtualization, it requires that a hypervisor is run on the original OS to create the necessary environment for the virtualized OS. This hypervisor in turn simulates a kernel and the virtualized OS in full, and finally, the intended executable can run on the virtualized OS. As such, each virtualized OS or application requires its own hypervisor, simulated kernel, disk storage, RAM allocation, etc; to run. The technology Docker arose as a more efficient solution to the problem of serving a wide range of technological necessities while maintaining efficiency. In the next step, we will dive into some introductory Docker.